

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Saving and Loading Models &mdash; PyTorch Tutorials 0.5.0a0+aebf3b4 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/pytorch_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Vision" href="../pytorch_applied/vision.html" />
    <link rel="prev" title="Custom C++ and CUDA Extensions" href="../advanced/cpp_extension.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyTorch Tutorials
          

          
            
            <img src="../_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.5.0a0+aebf3b4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html">What is PyTorch?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#getting-started">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#tensors">Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#operations">Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#numpy-bridge">NumPy Bridge</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#converting-a-torch-tensor-to-a-numpy-array">Converting a Torch Tensor to a NumPy Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#converting-numpy-array-to-torch-tensor">Converting NumPy Array to Torch Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html">Autograd: automatic differentiation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html#tensor">Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/autograd_tutorial.html#gradients">Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html">Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#define-the-network">Define the network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#loss-function">Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#backprop">Backprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/neural_networks_tutorial.html#update-the-weights">Update the weights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html">Training a classifier</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#what-about-data">What about data?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#training-an-image-classifier">Training an image classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#loading-and-normalizing-cifar10">1. Loading and normalizing CIFAR10</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#define-a-convolution-neural-network">2. Define a Convolution Neural Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer">3. Define a Loss function and optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#train-the-network">4. Train the network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#test-the-network-on-the-test-data">5. Test the network on the test data</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#training-on-gpu">Training on GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#training-on-multiple-gpus">Training on multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/cifar10_tutorial.html#where-do-i-go-next">Where do I go next?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html">Optional: Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#imports-and-parameters">Imports and parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#dummy-dataset">Dummy DataSet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#simple-model">Simple Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#create-model-and-dataparallel">Create Model and DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#run-the-model">Run the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#results">Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#gpus">2 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#id1">3 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#id2">8 GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/blitz/data_parallel_tutorial.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/former_torchies_tutorial.html">PyTorch for former Torch users</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#inplace-out-of-place">Inplace / Out-of-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#zero-indexing">Zero Indexing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#no-camel-casing">No camel casing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#numpy-bridge">Numpy Bridge</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#converting-torch-tensor-to-numpy-array">Converting torch Tensor to numpy Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#converting-numpy-array-to-torch-tensor">Converting numpy Array to torch Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html#tensors-that-track-history">Tensors that track history</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/autograd_tutorial.html#gradients">Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html">nn package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#example-1-convnet">Example 1: ConvNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks">Forward and Backward Function Hooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net">Example 2: Recurrent Net</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html">Multi-GPU examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html#dataparallel">DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/former_torchies/parallelism_tutorial.html#part-of-the-model-on-cpu-and-part-on-the-gpu">Part of the model on CPU and part on the GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#warm-up-numpy">Warm-up: numpy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-tensors">PyTorch: Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#autograd">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd">PyTorch: Tensors and autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions">PyTorch: Defining new autograd functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#tensorflow-static-graphs">TensorFlow: Static Graphs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#nn-module"><cite>nn</cite> module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-nn">PyTorch: nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-optim">PyTorch: optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-custom-nn-modules">PyTorch: Custom nn Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#pytorch-control-flow-weight-sharing">PyTorch: Control Flow + Weight Sharing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/pytorch_with_examples.html#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id1">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_tensor/two_layer_net_numpy.html">Warm-up: numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_tensor/two_layer_net_tensor.html">PyTorch: Tensors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id2">Autograd</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/two_layer_net_autograd.html">PyTorch: Tensors and autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch: Defining new autograd functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_autograd/tf_two_layer_net.html">TensorFlow: Static Graphs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/pytorch_with_examples.html#id3"><cite>nn</cite> module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_nn.html">PyTorch: nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_optim.html">PyTorch: optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/two_layer_net_module.html">PyTorch: Custom nn Modules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/examples_nn/dynamic_net.html">PyTorch: Control Flow + Weight Sharing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#load-data">Load Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#visualize-a-few-images">Visualize a few images</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#training-the-model">Training the model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#visualizing-the-model-predictions">Visualizing the model predictions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#finetuning-the-convnet">Finetuning the convnet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#train-and-evaluate">Train and evaluate</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor">ConvNet as fixed feature extractor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html#id1">Train and evaluate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/data_loading_tutorial.html">Data Loading and Processing Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#dataset-class">Dataset class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#transforms">Transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/data_loading_tutorial.html#compose-transforms">Compose transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#iterating-through-the-dataset">Iterating through the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/data_loading_tutorial.html#afterword-torchvision">Afterword: torchvision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html">Introduction to PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#introduction-to-torch-s-tensor-library">Introduction to Torch’s tensor library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#creating-tensors">Creating Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#operations-with-tensors">Operations with Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#reshaping-tensors">Reshaping Tensors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/pytorch_tutorial.html#computation-graphs-and-automatic-differentiation">Computation Graphs and Automatic Differentiation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html">Deep Learning with PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives">Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#affine-maps">Affine Maps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#non-linearities">Non-Linearities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#softmax-and-probabilities">Softmax and Probabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#objective-functions">Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#optimization-and-training">Optimization and Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#creating-network-components-in-pytorch">Creating Network Components in PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier">Example: Logistic Regression Bag-of-Words classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html">Word Embeddings: Encoding Lexical Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings">Getting Dense Word Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#word-embeddings-in-pytorch">Word Embeddings in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling">An Example: N-Gram Language Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words">Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html">Sequence Models and Long-Short Term Memory Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch">LSTM’s in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging">Example: An LSTM for Part-of-Speech Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/sequence_models_tutorial.html#exercise-augmenting-the-lstm-part-of-speech-tagger-with-character-level-features">Exercise: Augmenting the LSTM part-of-speech tagger with character-level features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#dynamic-versus-static-deep-learning-toolkits">Dynamic versus Static Deep Learning Toolkits</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion">Bi-LSTM Conditional Random Field Discussion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/nlp/advanced_tutorial.html#exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hybrid_frontend_tutorial.html">Hybrid Frontend Tutorials</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html">Finetuning Torchvision Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#inputs">Inputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#helper-functions">Helper Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code">Model Training and Validation Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#set-model-parameters-requires-grad-attribute">Set Model Parameters’ .requires_grad attribute</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks">Initialize and Reshape the Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#resnet">Resnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#alexnet">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#vgg">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#squeezenet">Squeezenet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#densenet">Densenet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#load-data">Load Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#create-the-optimizer">Create the Optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#run-training-and-validation-step">Run Training and Validation Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#comparison-with-model-trained-from-scratch">Comparison with Model Trained from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../beginner/finetuning_torchvision_models_tutorial.html#final-thoughts-and-where-to-go-next">Final Thoughts and Where to Go Next</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#preparing-the-data">Preparing the Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#turning-names-into-tensors">Turning Names into Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#plotting-the-results">Plotting the Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#evaluating-the-results">Evaluating the Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#running-on-user-input">Running on User Input</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#preparing-the-data">Preparing the Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#creating-the-network">Creating the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#preparing-for-training">Preparing for Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#training-the-network">Training the Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#plotting-the-losses">Plotting the Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#sampling-the-network">Sampling the Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#loading-data-files">Loading data files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model">The Seq2Seq Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-encoder">The Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#the-decoder">The Decoder</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#simple-decoder">Simple Decoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#attention-decoder">Attention Decoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#preparing-training-data">Preparing Training Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training-the-model">Training the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#plotting-results">Plotting results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#training-and-evaluating">Training and Evaluating</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#visualizing-attention">Visualizing Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#replay-memory">Replay Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#dqn-algorithm">DQN algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#q-network">Q-network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#input-extraction">Input extraction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#hyperparameters-and-utilities">Hyperparameters and utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html#training-loop">Training loop</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#point-to-point-communication">Point-to-Point Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#collective-communication">Collective Communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#distributed-training">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#our-own-ring-allreduce">Our Own Ring-Allreduce</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/dist_tuto.html#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#communication-backends">Communication Backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intermediate/dist_tuto.html#initialization-methods">Initialization Methods</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#loading-the-data">Loading the data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#depicting-spatial-transformer-networks">Depicting spatial transformer networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#training-the-model">Training the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html#visualizing-the-stn-results">Visualizing the STN results</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/neural_style_tutorial.html">Neural Transfer Using PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#underlying-principle">Underlying Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#importing-packages-and-selecting-a-device">Importing Packages and Selecting a Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#loading-the-images">Loading the Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#loss-functions">Loss Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#content-loss">Content Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/neural_style_tutorial.html#style-loss">Style Loss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#importing-the-model">Importing the Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/neural_style_tutorial.html#gradient-descent">Gradient Descent</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parameter-less-example">Parameter-less example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html#parametrized-example">Parametrized example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#transfering-srresnet-using-onnx">Transfering SRResNet using ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html#running-the-model-on-mobile-devices">Running the model on mobile devices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced/cpp_extension.html#motivation-and-example">Motivation and Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/cpp_extension.html#writing-a-c-extension">Writing a C++ Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#building-with-setuptools">Building with <code class="docutils literal notranslate"><span class="pre">setuptools</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#writing-the-c-op">Writing the C++ Op</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/cpp_extension.html#forward-pass">Forward Pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../advanced/cpp_extension.html#backward-pass">Backward Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#binding-to-python">Binding to Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#using-your-extension">Using Your Extension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/cpp_extension.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l4"><a class="reference internal" href="../advanced/cpp_extension.html#performance-on-gpu-devices">Performance on GPU Devices</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#jit-compiling-extensions">JIT Compiling Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">Writing a Mixed C++/CUDA extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../advanced/cpp_extension.html#integrating-a-c-cuda-operation-with-pytorch">Integrating a C++/CUDA Operation with PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../advanced/cpp_extension.html#id2">Performance Comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../advanced/cpp_extension.html#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">PyTorch Basics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Saving and Loading Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-a-state-dict">What is a <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example">Example:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#saving-loading-model-for-inference">Saving &amp; Loading Model for Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#save-load-state-dict-recommended">Save/Load <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (Recommended)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#save-load-entire-model">Save/Load Entire Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training">Saving &amp; Loading a General Checkpoint for Inference and/or Resuming Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#save">Save:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load">Load:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#saving-multiple-models-in-one-file">Saving Multiple Models in One File</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Save:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Load:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#warmstarting-model-using-parameters-from-a-different-model">Warmstarting Model Using Parameters from a Different Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">Save:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">Load:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#saving-loading-model-across-devices">Saving &amp; Loading Model Across Devices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#save-on-gpu-load-on-cpu">Save on GPU, Load on CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#save-on-gpu-load-on-gpu">Save on GPU, Load on GPU</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#save-on-cpu-load-on-gpu">Save on CPU, Load on GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-torch-nn-dataparallel-models">Saving <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> Models</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">PyTorch Applied</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_applied/vision.html">Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_applied/audio.html">Audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_applied/language.html">Language/NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_applied/language.html#character-level-rnns">Character-Level RNNs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_applied/language.html#seq2seq">Seq2Seq</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">PyTorch in Depth</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/distributed.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/custom_collate_functions.html">Custom Collate Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/custom_data_loading.html">Custom Data Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/custom_operators.html">Custom Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/graph_mode.html">Graph Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_in_depth/tensor_comprehensions.html">Tensor Comprehensions</a></li>
</ul>
<p class="caption"><span class="caption-text">Hacking PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hacking_pytorch/cpp_extensions.html">CPP Extensions</a></li>
</ul>
<p class="caption"><span class="caption-text">Installation Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation_guides/local.html">Local Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/linux.html">Linux Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/windows.html">Windows Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/osx.html">OSX Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../installation_guides/cloud.html">Cloud Installations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/gce.html">Google Compute Engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/aws.html">Amazon Web Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation_guides/microsoft_azure.html">Microsoft Azure</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyTorch Tutorials</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Saving and Loading Models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/pytorch_basics/saving_loading_models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-pytorch-basics-saving-loading-models-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="saving-and-loading-models">
<span id="sphx-glr-pytorch-basics-saving-loading-models-py"></span><h1>Saving and Loading Models<a class="headerlink" href="#saving-and-loading-models" title="Permalink to this headline">¶</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></p>
<p>This document provides solutions to a variety of use cases regarding the
saving and loading of PyTorch models. Feel free to read the whole
document, or just skip to the code you need for a desired use case.</p>
<p>When it comes to saving and loading models, there are three core
functions to be familiar with:</p>
<ol class="arabic simple">
<li><a class="reference external" href="https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save">torch.save</a>:
Saves a serialized object to disk. This function uses Python’s
<a class="reference external" href="https://docs.python.org/3/library/pickle.html">pickle</a> utility
for serialization. Models, tensors, and dictionaries of all kinds of
objects can be saved using this function.</li>
<li><a class="reference external" href="https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load">torch.load</a>:
Uses <a class="reference external" href="https://docs.python.org/3/library/pickle.html">pickle</a>’s
unpickling facilities to deserialize pickled object files to memory.
This function also facilitates the device to load the data into (see
<a class="reference external" href="#saving-loading-model-across-devices">Saving &amp; Loading Model Across
Devices</a>).</li>
<li><a class="reference external" href="https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict">torch.nn.Module.load_state_dict</a>:
Loads a model’s parameter dictionary using a deserialized
<em>state_dict</em>. For more information on <em>state_dict</em>, see <a class="reference external" href="#what-is-a-state-dict">What is a
state_dict?</a>.</li>
</ol>
<p><strong>Contents:</strong></p>
<ul class="simple">
<li><a class="reference external" href="#what-is-a-state-dict">What is a state_dict?</a></li>
<li><a class="reference external" href="#saving-loading-model-for-inference">Saving &amp; Loading Model for
Inference</a></li>
<li><a class="reference external" href="#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training">Saving &amp; Loading a General
Checkpoint</a></li>
<li><a class="reference external" href="#saving-multiple-models-in-one-file">Saving Multiple Models in One
File</a></li>
<li><a class="reference external" href="#warmstarting-model-using-parameters-from-a-different-model">Warmstarting Model Using Parameters from a Different
Model</a></li>
<li><a class="reference external" href="#saving-loading-model-across-devices">Saving &amp; Loading Model Across
Devices</a></li>
</ul>
<div class="section" id="what-is-a-state-dict">
<h2>What is a <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>?<a class="headerlink" href="#what-is-a-state-dict" title="Permalink to this headline">¶</a></h2>
<p>In PyTorch, the learnable parameters (i.e.&nbsp;weights and biases) of an
<code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> model is contained in the model’s <em>parameters</em>
(accessed with <code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code>). A <em>state_dict</em> is simply a
Python dictionary object that maps each layer to its parameter tensor.
Note that only layers with learnable parameters (convolutional layers,
linear layers, etc.) have entries in the model’s <em>state_dict</em>. Optimizer
objects (<code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>) also have a <em>state_dict</em>, which contains
information about the optimizer’s state, as well as the hyperparameters
used.</p>
<p>Because <em>state_dict</em> objects are Python dictionaries, they can be easily
saved, updated, altered, and restored, adding a great deal of modularity
to PyTorch models and optimizers.</p>
<div class="section" id="example">
<h3>Example:<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>Let’s take a look at the <em>state_dict</em> from the simple model used in the
<a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">Training a
classifier</a>
tutorial.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define model</span>
<span class="k">class</span> <span class="nc">TheModelClass</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TheModelClass</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">()</span>

<span class="c1"># Initialize optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Print model&#39;s state_dict</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model&#39;s state_dict:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param_tensor</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">param_tensor</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="n">param_tensor</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c1"># Print optimizer&#39;s state_dict</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimizer&#39;s state_dict:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="n">var_name</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Output:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="s1">&#39;s state_dict:</span>
<span class="n">conv1</span><span class="o">.</span><span class="n">weight</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">conv1</span><span class="o">.</span><span class="n">bias</span>   <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="n">conv2</span><span class="o">.</span><span class="n">weight</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">conv2</span><span class="o">.</span><span class="n">bias</span>   <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">])</span>
<span class="n">fc1</span><span class="o">.</span><span class="n">weight</span>   <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">120</span><span class="p">,</span> <span class="mi">400</span><span class="p">])</span>
<span class="n">fc1</span><span class="o">.</span><span class="n">bias</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">120</span><span class="p">])</span>
<span class="n">fc2</span><span class="o">.</span><span class="n">weight</span>   <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">84</span><span class="p">,</span> <span class="mi">120</span><span class="p">])</span>
<span class="n">fc2</span><span class="o">.</span><span class="n">bias</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">84</span><span class="p">])</span>
<span class="n">fc3</span><span class="o">.</span><span class="n">weight</span>   <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">84</span><span class="p">])</span>
<span class="n">fc3</span><span class="o">.</span><span class="n">bias</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>

<span class="n">Optimizer</span><span class="s1">&#39;s state_dict:</span>
<span class="n">state</span>    <span class="p">{}</span>
<span class="n">param_groups</span>     <span class="p">[{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;dampening&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;nesterov&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4675713712</span><span class="p">,</span> <span class="mi">4675713784</span><span class="p">,</span> <span class="mi">4675714000</span><span class="p">,</span> <span class="mi">4675714072</span><span class="p">,</span> <span class="mi">4675714216</span><span class="p">,</span> <span class="mi">4675714288</span><span class="p">,</span> <span class="mi">4675714432</span><span class="p">,</span> <span class="mi">4675714504</span><span class="p">,</span> <span class="mi">4675714648</span><span class="p">,</span> <span class="mi">4675714720</span><span class="p">]}]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-loading-model-for-inference">
<h2>Saving &amp; Loading Model for Inference<a class="headerlink" href="#saving-loading-model-for-inference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="save-load-state-dict-recommended">
<h3>Save/Load <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> (Recommended)<a class="headerlink" href="#save-load-state-dict-recommended" title="Permalink to this headline">¶</a></h3>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<p>When saving a model for inference, it is only necessary to save the
trained model’s learned parameters. Saving the model’s <em>state_dict</em> with
the <code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> function will give you the most flexibility for
restoring the model later, which is why it is the recommended method for
saving models.</p>
<p>A common PyTorch convention is to save models using either a <code class="docutils literal notranslate"><span class="pre">.pt</span></code> or
<code class="docutils literal notranslate"><span class="pre">.pth</span></code> file extension.</p>
<p>Remember that you must call <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> to set dropout and batch
normalization layers to evaluation mode before running inference.
Failing to do this will yield inconsistent inference results.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Notice that the <code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code> function takes a dictionary
object, NOT a path to a saved object. This means that you must
deserialize the saved <em>state_dict</em> before you pass it to the
<code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code> function. For example, you CANNOT load using
<code class="docutils literal notranslate"><span class="pre">model.load_state_dict(PATH)</span></code>.</p>
</div>
</div>
<div class="section" id="save-load-entire-model">
<h3>Save/Load Entire Model<a class="headerlink" href="#save-load-entire-model" title="Permalink to this headline">¶</a></h3>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model class must be defined somewhere</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<p>This save/load process uses the most intuitive syntax and involves the
least amount of code. Saving a model in this way will save the entire
module using Python’s
<a class="reference external" href="https://docs.python.org/3/library/pickle.html">pickle</a> module. The
disadvantage of this approach is that the serialized data is bound to
the specific classes and the exact directory structure used when the
model is saved. The reason for this is because pickle does not save the
model class itself. Rather, it saves a path to the file containing the
class, which is used during load time. Because of this, your code can
break in various ways when used in other projects or after refactors.</p>
<p>A common PyTorch convention is to save models using either a <code class="docutils literal notranslate"><span class="pre">.pt</span></code> or
<code class="docutils literal notranslate"><span class="pre">.pth</span></code> file extension.</p>
<p>Remember that you must call <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> to set dropout and batch
normalization layers to evaluation mode before running inference.
Failing to do this will yield inconsistent inference results.</p>
</div>
</div>
<div class="section" id="saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training">
<h2>Saving &amp; Loading a General Checkpoint for Inference and/or Resuming Training<a class="headerlink" href="#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="save">
<h3>Save:<a class="headerlink" href="#save" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s1">&#39;model_state_dict&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
            <span class="o">...</span>
            <span class="p">},</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="load">
<h3>Load:<a class="headerlink" href="#load" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">TheOptimizerClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">])</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">])</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># - or -</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>When saving a general checkpoint, to be used for either inference or
resuming training, you must save more than just the model’s
<em>state_dict</em>. It is important to also save the optimizer’s <em>state_dict</em>,
as this contains buffers and parameters that are updated as the model
trains. Other items that you may want to save are the epoch you left off
on, the latest recorded training loss, external <code class="docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code>
layers, etc.</p>
<p>To save multiple components, organize them in a dictionary and use
<code class="docutils literal notranslate"><span class="pre">torch.save()</span></code> to serialize the dictionary. A common PyTorch
convention is to save these checkpoints using the <code class="docutils literal notranslate"><span class="pre">.tar</span></code> file
extension.</p>
<p>To load the items, first initialize the model and optimizer, then load
the dictionary locally using <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code>. From here, you can easily
access the saved items by simply querying the dictionary as you would
expect.</p>
<p>Remember that you must call <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> to set dropout and batch
normalization layers to evaluation mode before running inference.
Failing to do this will yield inconsistent inference results. If you
wish to resuming training, call <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> to ensure these layers
are in training mode.</p>
</div>
</div>
<div class="section" id="saving-multiple-models-in-one-file">
<h2>Saving Multiple Models in One File<a class="headerlink" href="#saving-multiple-models-in-one-file" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Save:<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
            <span class="s1">&#39;modelA_state_dict&#39;</span><span class="p">:</span> <span class="n">modelA</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;modelB_state_dict&#39;</span><span class="p">:</span> <span class="n">modelB</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizerA_state_dict&#39;</span><span class="p">:</span> <span class="n">optimizerA</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s1">&#39;optimizerB_state_dict&#39;</span><span class="p">:</span> <span class="n">optimizerB</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="o">...</span>
            <span class="p">},</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>Load:<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">modelA</span> <span class="o">=</span> <span class="n">TheModelAClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">modelB</span> <span class="o">=</span> <span class="n">TheModelBClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">optimizerA</span> <span class="o">=</span> <span class="n">TheOptimizerAClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">optimizerB</span> <span class="o">=</span> <span class="n">TheOptimizerBClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">modelA</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;modelA_state_dict&#39;</span><span class="p">])</span>
<span class="n">modelB</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;modelB_state_dict&#39;</span><span class="p">])</span>
<span class="n">optimizerA</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizerA_state_dict&#39;</span><span class="p">])</span>
<span class="n">optimizerB</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;optimizerB_state_dict&#39;</span><span class="p">])</span>

<span class="n">modelA</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">modelB</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># - or -</span>
<span class="n">modelA</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">modelB</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>When saving a model comprised of multiple <code class="docutils literal notranslate"><span class="pre">torch.nn.Modules</span></code>, such as
a GAN, a sequence-to-sequence model, or an ensemble of models, you
follow the same approach as when you are saving a general checkpoint. In
other words, save a dictionary of each model’s <em>state_dict</em> and
corresponding optimizer. As mentioned before, you can save any other
items that may aid you in resuming training by simply appending them to
the dictionary.</p>
<p>A common PyTorch convention is to save these checkpoints using the
<code class="docutils literal notranslate"><span class="pre">.tar</span></code> file extension.</p>
<p>To load the models, first initialize the models and optimizers, then
load the dictionary locally using <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code>. From here, you can
easily access the saved items by simply querying the dictionary as you
would expect.</p>
<p>Remember that you must call <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> to set dropout and batch
normalization layers to evaluation mode before running inference.
Failing to do this will yield inconsistent inference results. If you
wish to resuming training, call <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> to set these layers to
training mode.</p>
</div>
</div>
<div class="section" id="warmstarting-model-using-parameters-from-a-different-model">
<h2>Warmstarting Model Using Parameters from a Different Model<a class="headerlink" href="#warmstarting-model-using-parameters-from-a-different-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>Save:<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">modelA</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h3>Load:<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">modelB</span> <span class="o">=</span> <span class="n">TheModelBClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">modelB</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Partially loading a model or loading a partial model are common
scenarios when transfer learning or training a new complex model.
Leveraging trained parameters, even if only a few are usable, will help
to warmstart the training process and hopefully help your model converge
much faster than training from scratch.</p>
<p>Whether you are loading from a partial <em>state_dict</em>, which is missing
some keys, or loading a <em>state_dict</em> with more keys than the model that
you are loading into, you can set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument to <strong>False</strong>
in the <code class="docutils literal notranslate"><span class="pre">load_state_dict()</span></code> function to ignore non-matching keys.</p>
<p>If you want to load parameters from one layer to another, but some keys
do not match, simply change the name of the parameter keys in the
<em>state_dict</em> that you are loading to match the keys in the model that
you are loading into.</p>
</div>
</div>
<div class="section" id="saving-loading-model-across-devices">
<h2>Saving &amp; Loading Model Across Devices<a class="headerlink" href="#saving-loading-model-across-devices" title="Permalink to this headline">¶</a></h2>
<div class="section" id="save-on-gpu-load-on-cpu">
<h3>Save on GPU, Load on CPU<a class="headerlink" href="#save-on-gpu-load-on-cpu" title="Permalink to this headline">¶</a></h3>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
<p>When loading a model on a CPU that was trained with a GPU, pass
<code class="docutils literal notranslate"><span class="pre">torch.device('cpu')</span></code> to the <code class="docutils literal notranslate"><span class="pre">map_location</span></code> argument in the
<code class="docutils literal notranslate"><span class="pre">torch.load()</span></code> function. In this case, the storages underlying the
tensors are dynamically remapped to the CPU device using the
<code class="docutils literal notranslate"><span class="pre">map_location</span></code> argument.</p>
</div>
<div class="section" id="save-on-gpu-load-on-gpu">
<h3>Save on GPU, Load on GPU<a class="headerlink" href="#save-on-gpu-load-on-gpu" title="Permalink to this headline">¶</a></h3>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span>
</pre></div>
</div>
<p>When loading a model on a GPU that was trained and saved on GPU, simply
convert the initialized <code class="docutils literal notranslate"><span class="pre">model</span></code> to a CUDA optimized model using
<code class="docutils literal notranslate"><span class="pre">model.to(torch.device('cuda'))</span></code>. Also, be sure to use the
<code class="docutils literal notranslate"><span class="pre">.to(torch.device('cuda'))</span></code> function on all model inputs to prepare
the data for the model. Note that calling <code class="docutils literal notranslate"><span class="pre">my_tensor.to(device)</span></code>
returns a new copy of <code class="docutils literal notranslate"><span class="pre">my_tensor</span></code> on GPU. It does NOT overwrite
<code class="docutils literal notranslate"><span class="pre">my_tensor</span></code>. Therefore, remember to manually overwrite tensors:
<code class="docutils literal notranslate"><span class="pre">my_tensor</span> <span class="pre">=</span> <span class="pre">my_tensor.to(torch.device('cuda'))</span></code>.</p>
<div class="section" id="save-on-cpu-load-on-gpu">
<h4>Save on CPU, Load on GPU<a class="headerlink" href="#save-on-cpu-load-on-gpu" title="Permalink to this headline">¶</a></h4>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">))</span>  <span class="c1"># Choose whatever GPU device number you want</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span>
</pre></div>
</div>
<p>When loading a model on a GPU that was trained and saved on CPU, set the
<code class="docutils literal notranslate"><span class="pre">map_location</span></code> argument in the <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code> function to
<em>cuda:device_id</em>. This loads the model to a given GPU device. Next, be
sure to call <code class="docutils literal notranslate"><span class="pre">model.to(torch.device('cuda'))</span></code> to convert the model’s
parameter tensors to CUDA tensors. Finally, be sure to use the
<code class="docutils literal notranslate"><span class="pre">.to(torch.device('cuda'))</span></code> function on all model inputs to prepare
the data for the CUDA optimized model. Note that calling
<code class="docutils literal notranslate"><span class="pre">my_tensor.to(device)</span></code> returns a new copy of <code class="docutils literal notranslate"><span class="pre">my_tensor</span></code> on GPU. It
does NOT overwrite <code class="docutils literal notranslate"><span class="pre">my_tensor</span></code>. Therefore, remember to manually
overwrite tensors: <code class="docutils literal notranslate"><span class="pre">my_tensor</span> <span class="pre">=</span> <span class="pre">my_tensor.to(torch.device('cuda'))</span></code>.</p>
</div>
<div class="section" id="saving-torch-nn-dataparallel-models">
<h4>Saving <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> Models<a class="headerlink" href="#saving-torch-nn-dataparallel-models" title="Permalink to this headline">¶</a></h4>
<p><strong>Save:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Load:</strong></p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load to whatever device you want</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> is a model wrapper that enables parallel GPU
utilization. To save a <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code> model generically, save the
<code class="docutils literal notranslate"><span class="pre">model.module.state_dict()</span></code>. This way, you have the flexibility to
load the model any way you want to any device you want.</p>
<p><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-pytorch-basics-saving-loading-models-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../_downloads/saving_loading_models.py" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">saving_loading_models.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../_downloads/saving_loading_models.ipynb" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">saving_loading_models.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../pytorch_applied/vision.html" class="btn btn-neutral float-right" title="Vision" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../advanced/cpp_extension.html" class="btn btn-neutral" title="Custom C++ and CUDA Extensions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.5.0a0+aebf3b4',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-2', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>